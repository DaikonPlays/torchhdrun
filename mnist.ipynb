{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e46043f-5608-4e6f-9bd3-6b738fc04737",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision\n",
    "import sklearn.metrics\n",
    "from tqdm import tqdm\n",
    "\n",
    "import hdc\n",
    "import hdc.functional as HDF\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fec3569-684a-4982-9973-9b2a3e041e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device)\n",
    "print(\"Using {} device\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e450c5b4-fbd2-41f0-a7d9-d6fe3ae5f7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIMENSIONS = 10000\n",
    "IMG_SIZE = 28\n",
    "NUM_LEVELS = 1000\n",
    "BATCH_SIZE = 12\n",
    "LEARNING_RATE = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5177e5ef-0c8a-4bef-acd3-f97472591226",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9607bd73-4230-4285-a7c4-53f0d46ee1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9913344it [00:00, 57191637.11it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29696it [00:00, 17134963.76it/s]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1649664it [00:00, 49937154.94it/s]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5120it [00:00, 15538955.48it/s]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(60000, 10000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = torchvision.datasets.MNIST(\"data\", train=True, transform=transform, download=True)\n",
    "train_ld = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_ds = torchvision.datasets.MNIST(\"data\", train=False, transform=transform, download=True)\n",
    "test_ld = torch.utils.data.DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "len(train_ds), len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fca355b-5d1a-4445-8387-2d0145bfafdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = len(train_ds.classes)\n",
    "POS_HV = HDF.random_hv(IMG_SIZE * IMG_SIZE, DIMENSIONS, dtype=torch.float, device=device)\n",
    "LUM_HV = HDF.level_hv(NUM_LEVELS, DIMENSIONS, dtype=torch.float, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a87cc81f-6b71-4f10-ba0a-a50fc78de17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, size, train_embedding=False):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.size = size\n",
    "\n",
    "        self.pos_embed = hdc.embeddings.Random(size * size, DIMENSIONS)\n",
    "        self.pos_embed.weight.requires_grad = train_embedding\n",
    "        \n",
    "        self.lum_embed = hdc.embeddings.Level(NUM_LEVELS, DIMENSIONS)\n",
    "        self.lum_embed.weight.requires_grad = train_embedding\n",
    "        \n",
    "        self.classify = nn.Linear(DIMENSIONS, NUM_CLASSES)\n",
    "        self.classify.weight.data.fill_(0.0)\n",
    "        self.classify.bias.data.fill_(0.0)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = x.reshape(batch_size, self.size * self.size)\n",
    "\n",
    "        luminocities = self.lum_embed(x)\n",
    "        \n",
    "        sample_hv = HDF.bind(self.pos_embed.weight, luminocities)\n",
    "        sample_hv = torch.sum(sample_hv, dim=-2)\n",
    "\n",
    "        return HDF.soft_quantize(sample_hv)  # cap between -1 and +1\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc = self.encode(x)\n",
    "        logit = self.classify(enc)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db5c88de-caa6-44cb-a5ab-3367231701d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(IMG_SIZE)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2362508-bc33-45a4-929f-6fdc92bddd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5000/5000 [00:30<00:00, 162.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took 30.744s for 60000 items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for samples, labels in tqdm(train_ld, desc=\"Train\"):\n",
    "        samples = samples.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        samples_hv = model.encode(samples)\n",
    "        model.classify.weight.data[labels] += samples_hv * LEARNING_RATE\n",
    "\n",
    "    model.classify.weight.data = F.normalize(model.classify.weight.data)\n",
    "    \n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "print(f\"Training took {duration:.3f}s for {len(train_ds)} items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b9ef0cc-6217-4bd1-972c-75e77ea353fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 834/834 [00:06<00:00, 136.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing took 6.10s for 10000 items\n",
      "Testing accuracy of 82.580%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pred_labels = []\n",
    "true_labels = []\n",
    "\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    for samples, labels in tqdm(test_ld, desc=\"Test\"):\n",
    "        samples = samples.to(device)\n",
    "\n",
    "        outputs = model(samples)\n",
    "        predictions = torch.argmax(outputs, dim=-1)\n",
    "\n",
    "        pred_labels.append(predictions)\n",
    "        true_labels.append(labels)\n",
    "        \n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "print(f\"Testing took {duration:.2f}s for {len(test_ds)} items\")\n",
    "\n",
    "pred_labels = torch.cat(pred_labels).cpu()\n",
    "true_labels = torch.cat(true_labels).cpu()\n",
    "\n",
    "accuracy = sklearn.metrics.accuracy_score(pred_labels, true_labels)\n",
    "print(f\"Testing accuracy of {(accuracy * 100):.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49021a29-a65e-449e-ba68-6faf2bce0749",
   "metadata": {},
   "source": [
    "## Gradient-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a09b5902-5b88-4a0f-9143-6a3132048105",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(IMG_SIZE)\n",
    "model = model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cdbbbbc1-3050-4ac7-8047-3fe7074cda61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5000/5000 [00:32<00:00, 153.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took 32.526s for 60000 items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for samples, labels in tqdm(train_ld, desc=\"Train\"):\n",
    "    samples = samples.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(samples)\n",
    "    \n",
    "    loss = loss_fn(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "print(f\"Training took {duration:.3f}s for {len(train_ds)} items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5627b051-17ca-4364-b097-d01959fa974f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 834/834 [00:05<00:00, 160.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing took 5.22s for 10000 items\n",
      "Testing accuracy of 89.610%\n"
     ]
    }
   ],
   "source": [
    "pred_labels = []\n",
    "true_labels = []\n",
    "\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    for samples, labels in tqdm(test_ld, desc=\"Test\"):\n",
    "        samples = samples.to(device)\n",
    "\n",
    "        outputs = model(samples)\n",
    "        predictions = torch.argmax(outputs, dim=-1)\n",
    "\n",
    "        pred_labels.append(predictions)\n",
    "        true_labels.append(labels)\n",
    "        \n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "print(f\"Testing took {duration:.2f}s for {len(test_ds)} items\")\n",
    "\n",
    "pred_labels = torch.cat(pred_labels).cpu()\n",
    "true_labels = torch.cat(true_labels).cpu()\n",
    "\n",
    "accuracy = sklearn.metrics.accuracy_score(pred_labels, true_labels)\n",
    "print(f\"Testing accuracy of {(accuracy * 100):.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0125eb-5296-46fd-a845-88f0cb5d50ac",
   "metadata": {},
   "source": [
    "## Mixed HDC and gradient-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "622248bc-edd6-42e3-81d8-7227b46c92d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(IMG_SIZE)\n",
    "model = model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bf697119-3b7e-499f-87ea-35a354188e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train: 100%|██████████| 5000/5000 [00:54<00:00, 91.55it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took 54.624s for 60000 items\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "cache = torch.zeros(NUM_CLASSES, DIMENSIONS, device=device, dtype=torch.float)\n",
    "dirty_bit = torch.tensor([False] * NUM_CLASSES, dtype=torch.bool, device=device)\n",
    "for samples, labels in tqdm(train_ld, desc=\"Train\"):\n",
    "    samples = samples.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    if random.random() > 0.9:\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        enc = model.encode(samples)\n",
    "        for l in range(labels.size(0)):\n",
    "            enc[l] = HDF.bundle(enc[l], cache[labels[l]])\n",
    "            cache[labels[l]] = 0\n",
    "            dirty_bit[labels[l]] = False\n",
    "            \n",
    "        outputs = model.classify(enc)\n",
    "\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            samples_hv = model.encode(samples)\n",
    "            for l in range(samples_hv.size(0)):\n",
    "                cache[labels[l]] += samples_hv[l]\n",
    "                dirty_bit[labels[l]] = True\n",
    "    \n",
    "# Apply all accumulated samples                      \n",
    "# zero the parameter gradients\n",
    "optimizer.zero_grad()\n",
    "\n",
    "outputs = model.classify(cache[dirty_bit])\n",
    "labels = torch.arange(0, NUM_CLASSES, device=device, dtype=torch.long)\n",
    "labels = labels[dirty_bit]\n",
    "\n",
    "loss = loss_fn(outputs, labels)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "print(f\"Training took {duration:.3f}s for {len(train_ds)} items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3dc65934-bace-41be-9913-0fd317b02814",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 834/834 [00:05<00:00, 160.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing took 5.19s for 10000 items\n",
      "Testing accuracy of 84.010%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pred_labels = []\n",
    "true_labels = []\n",
    "\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    for samples, labels in tqdm(test_ld, desc=\"Test\"):\n",
    "        samples = samples.to(device)\n",
    "\n",
    "        outputs = model(samples)\n",
    "        predictions = torch.argmax(outputs, dim=-1)\n",
    "\n",
    "        pred_labels.append(predictions)\n",
    "        true_labels.append(labels)\n",
    "        \n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "print(f\"Testing took {duration:.2f}s for {len(test_ds)} items\")\n",
    "\n",
    "pred_labels = torch.cat(pred_labels).cpu()\n",
    "true_labels = torch.cat(true_labels).cpu()\n",
    "\n",
    "accuracy = sklearn.metrics.accuracy_score(pred_labels, true_labels)\n",
    "print(f\"Testing accuracy of {(accuracy * 100):.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807a6e56-d05b-4d74-9f0e-eadb9b49e451",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
